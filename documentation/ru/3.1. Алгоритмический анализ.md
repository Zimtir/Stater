# Алгоритмический анализ

_В данном документе представлен пункт 3.1 из раздела **Глава 3**_

- Асимптотическая сложность алгоритмов (коротко)
  - https://tproger.ru/articles/computational-complexity-explained/
  
- Скорость получения данных в BigData
  - https://new-retail.ru/tehnologii/kakie_tekhnologii_mogut_povysit_skorost_obrabotki_big_data_v_riteyle9416/
  - http://ru.datasides.com/big-data-in-economics/

- Метод перебора для предсказаний
  - https://cyberleninka.ru/article/n/metod-polnogo-perebora-v-zadache-mnogofaktornogo-regressionnogo-analiza

- Как долго работает генетический алгоритм
  - https://basegroup.ru/community/articles/real-coded-ga
  - http://www.tsi.lv/sites/default/files/editor/science/Research_journals/Tr_Tel/V1/6.pdf
  
  И всё это привязать к данным статистики
  
  <p>Сложность алгоритмов обычно оценивают по времени выполнения или по используемой памяти. В обоих случаях сложность зависит от размеров входных данных: массив из 100 элементов будет обработан быстрее, чем аналогичный из 1000. При этом точное время мало кого интересует: оно зависит от процессора, типа данных, языка программирования и множества других параметров. Важна лишь асимптотическая сложность, т. е. сложность при стремлении размера входных данных к бесконечности.</p>
<p>Допустим, некоторому алгоритму нужно выполнить&nbsp;<code>4n<sup>3</sup>&nbsp;+ 7n</code>&nbsp;условных операций, чтобы обработать&nbsp;<code>n</code>&nbsp;элементов входных данных. При увеличении&nbsp;<code>n</code>&nbsp;на итоговое время работы будет значительно больше влиять возведение&nbsp;<code>n</code>&nbsp;в куб, чем умножение его на&nbsp;<code>4</code>&nbsp;или же прибавление&nbsp;<code>7n</code>. Тогда говорят, что временная сложность этого алгоритма равна&nbsp;<code>О(n<sup>3</sup>)</code>, т. е. зависит от размера входных данных кубически.</p>
<p>Использование заглавной буквы О (или так называемая О-нотация) пришло из математики, где её применяют для сравнения асимптотического поведения функций. Формально&nbsp;<code>O(f(n))</code>&nbsp;означает, что время работы алгоритма (или объём занимаемой памяти) растёт в зависимости от объёма входных данных не быстрее, чем некоторая константа, умноженная на&nbsp;<code>f(n)</code>.</p>
<p>&nbsp;</p>
<h2>Примеры</h2>
<h3>O(n) &mdash; линейная сложность</h3>
<p>Такой сложностью обладает, например, алгоритм поиска наибольшего элемента в не отсортированном массиве. Нам придётся пройтись по всем&nbsp;<code>n</code>&nbsp;элементам массива, чтобы понять, какой из них максимальный.</p>
<h3>O(log n) &mdash; логарифмическая сложность</h3>
<p>Простейший пример &mdash; бинарный поиск. Если массив отсортирован, мы можем проверить, есть ли в нём какое-то конкретное значение, методом деления пополам. Проверим средний элемент, если он больше искомого, то отбросим вторую половину массива &mdash; там его точно нет. Если же меньше, то наоборот &mdash; отбросим начальную половину. И так будем продолжать делить пополам, в итоге проверим&nbsp;<code>log n</code>&nbsp;элементов.</p>
<h3>O(n<sup>2</sup>) &mdash; квадратичная сложность</h3>
<p>Такую сложность имеет, например, алгоритм сортировки вставками. В канонической реализации он представляет из себя два вложенных цикла: один, чтобы проходить по всему массиву, а второй, чтобы находить место очередному элементу в уже отсортированной части. Таким образом, количество операций будет зависеть от размера массива как&nbsp;<code>n * n</code>, т. е.&nbsp;<code>n<sup>2</sup></code>.</p>
<p>Бывают и другие оценки по сложности, но все они основаны на том же принципе.</p>
<p>Также случается, что время работы алгоритма вообще не зависит от размера входных данных. Тогда сложность обозначают как&nbsp;<code>O(1)</code>. Например, для определения значения третьего элемента массива не нужно ни запоминать элементы, ни проходить по ним сколько-то раз. Всегда нужно просто дождаться в потоке входных данных третий элемент и это будет результатом, на вычисление которого для любого количества данных нужно одно и то же время.</p>
<p>Аналогично проводят оценку и по памяти, когда это важно. Однако алгоритмы могут использовать значительно больше памяти при увеличении размера входных данных, чем другие, но зато работать быстрее. И наоборот. Это помогает выбирать оптимальные пути решения задач исходя из текущих условий и требований.</p>

## Big data



